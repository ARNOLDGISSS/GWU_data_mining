## Section 04: Decision Trees

Decision trees strike a nice balance between interpretability and accuracy. They pick up on nonlinearity and high degree interactions, but they still produce simple rules or diagrams that explain their decisions. They're also a very robust modeling technique that can generally accept missing values, variables of disparate scales, and correlated variables.

#### Class Notes

* [Overview of decision trees](notes/instructor_notes.pdf)

* Overview of training decision trees in Enterprise Miner - [Blackboard electronic reserves](https://blackboard.gwu.edu)

* [More decision tree splitting and stopping strategies](notes/tan_notes.pdf)

* [EM decision tree example](xml/04_decision_trees.xml)

* [H2o decision tree ensemble examples](http://nbviewer.jupyter.org/github/jphall663/GWU_data_mining/blob/master/04_decision_trees/src/py_part_4_decision_tree_ensembles.ipynb)

#### [Sample Quiz](quiz/sample/quiz_4.pdf)

#### Supplementary References

* [*Introduction to Statistical Learning*](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf)</br>
Chapter 8

* [*Introduction to Data Mining*](http://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf)</br>
Chapter 4

* [*Gradient Boosting Machines with H2O*](http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/5/docs-website/h2o-docs/booklets/GBMBooklet.pdf)

* *Predictive Modeling and Decision Trees in Enterprise Miner* - [Blackboard electronic reserves](https://blackboard.gwu.edu)
